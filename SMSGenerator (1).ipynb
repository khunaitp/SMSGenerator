{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install Keras-Preprocessing"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t016fN7mylXV",
        "outputId": "14204c5e-5aef-4d92-8106-72dc90ffbd04"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Keras-Preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 KB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.8/dist-packages (from Keras-Preprocessing) (1.22.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from Keras-Preprocessing) (1.15.0)\n",
            "Installing collected packages: Keras-Preprocessing\n",
            "Successfully installed Keras-Preprocessing-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB1jFSN_Ok17",
        "outputId": "e7c480dc-277e-4818-f544-bc65e073d525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pythainlp in /usr/local/lib/python3.8/dist-packages (3.1.1)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.8/dist-packages (from pythainlp) (2.25.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.22.0->pythainlp) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.22.0->pythainlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.22.0->pythainlp) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.22.0->pythainlp) (4.0.0)\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 15.0M  100 15.0M    0     0  65.7M      0 --:--:-- --:--:-- --:--:-- 65.4M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 32295  100 32295    0     0   350k      0 --:--:-- --:--:-- --:--:--  350k\n"
          ]
        }
      ],
      "source": [
        "!pip install pythainlp\n",
        "!curl 'https://raw.githubusercontent.com/khunaitp/SMSGenerator/main/model.h5' > model.h5\n",
        "!curl 'https://raw.githubusercontent.com/khunaitp/SMSGenerator/main/tokenizer.pickle' > tokenizer.pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import numpy as np\n",
        "from pythainlp import word_tokenize\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "def generate_text(text, model, max_sequence_len):\n",
        "    seed_text = \" \".join(word_tokenize(text))\n",
        "    counter = 0\n",
        "    while counter < 100:\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predicted = model.predict(token_list, verbose=0)\n",
        "        predicted = np.random.choice(np.arange(0, predicted.shape[1]), p=predicted[0])\n",
        "\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \"+output_word\n",
        "        if output_word == \"END\":\n",
        "          joined_text = \"\".join(seed_text.split(\" \")[:-1])\n",
        "          return joined_text\n",
        "        counter += 1\n",
        "    return seed_text\n",
        "\n",
        "\n",
        "model = tf.keras.models.load_model('model.h5')\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2: type initiation sentence \n",
        "\n",
        "text = '\\u0E2A\\u0E27\\u0E31\\u0E2A\\u0E14\\u0E35' #@param {type:\"string\"}\n",
        "max_sequence_len = 51\n",
        "print(generate_text(text, model, 51))"
      ],
      "metadata": {
        "id": "dL3MiT1UPnyx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "6e884876-7ff8-430c-99be-328816389452"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "สวัสดีค่ะเครดิตของคุณเพิ่มเป็น100000บาท\n"
          ]
        }
      ]
    }
  ]
}